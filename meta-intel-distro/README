meta-intel-distro
=================

This is the OpenEmbedded/Yocto layer containing metadata to build a
full Intel Axxia reference platform for Victoria Canyon board (VCN)
for SNR machine, Travertine Fall board (TVF) for GRR machine and
simulation systems.

Details about supported BSPs and reference platforms in README.bsp.


Dependencies
============

This layer depends on meta-intel-bsp layer and its dependencies.

For a complete list of dependent layers, please check DEPENDENCIES.


Table of Contents
=================

 I. Prerequisites
II. Building meta-intel-distro
    1. Clone OpenEmbedded/Yocto layers
    2. Prepare and customize the build
    3. Select the image type and start the build
    4. Build and install the SDK
    5. Creating Bootable Disks for reference boards
    6. Read-only rootfs with writable overlayfs


I. Prerequisites
================

To begin using the Yocto Project build tools, you must first setup your
work environment and verify that you have the required host packages
installed on the system you will be using for builds. 

Check the YOCTO Reference Manual for the system you are using and verify
you have the minimum required packages installed:
http://www.yoctoproject.org/docs/current/ref-manual/ref-manual.html

Create an empty directory and verify that the partition has at least
50Gb of free space. Next set an environment variable, YOCTO, to the
full path.

   $ cd $HOME
   $ df -h .  # verify output shows adequate space available
   $ mkdir yocto
   $ cd yocto
   $ export YOCTO=$HOME/yocto # should also add to your ~/.bashrc file.


II. Building meta-intel-distro
==============================

1. Clone OpenEmbedded/Yocto layers
----------------------------------

1.1. Clone the Intel Axxia repository. This provides OpenEmbedded/Yocto
   layers with metadata for building images for the Intel Axxia specific
   reference platforms. See 'Sources' from the main README to select the
   right repository. Use the 'hardknott' branch or the tag specified in the
   release notes.

   $ cd $YOCTO
   $ git clone https://github.com/axxia/meta-intel-axxia_private.git \
               meta-intel-axxia
   OR
   $ git clone https://github.com/axxia/meta-intel-axxia.git
   $ cd meta-intel-axxia

   If using the 'hardknott' branch.

   $ git checkout hardknott

   If using the tag specified in the releases notes.

   $ git checkout tags/(the tag specified in the releases notes)

1.2. Clone the Yocto Project build tools (Poky) environment.

   $ cd $YOCTO
   $ git clone git://git.yoctoproject.org/poky.git
   $ cd poky
   $ git checkout <the revision specified in DEPENDENCIES>

1.3. The Open Embedded project provides many useful layers and packages
   such as networking. Download the Open Embedded Yocto Project hosted
   repository with the following.

   $ cd $YOCTO
   $ git clone https://github.com/openembedded/meta-openembedded.git
   $ cd meta-openembedded
   $ git checkout <the revision specified in DEPENDENCIES>

1.4. Clone Yocto Virtualization Layer which provides packages for
   virtualization such as Linux Container Support (lxc).

   $ cd $YOCTO
   $ git clone git://git.yoctoproject.org/meta-virtualization
   $ cd meta-virtualization
   $ git checkout <the revision specified in DEPENDENCIES>

1.5. Clone the Intel meta layer. This provides Intel hardware support 
   metadata which are inherited in Axxia BSPs.

   $ cd $YOCTO
   $ git clone git://git.yoctoproject.org/meta-intel
   $ cd meta-intel
   $ git checkout <the revision specified in DEPENDENCIES>

1.6. Clone the Yocto security meta layer. This provides recipes for some
   utilities used in the axxia-image-dev image.

   $ cd $YOCTO
   $ git clone git://git.yoctoproject.org/meta-security
   $ cd meta-security
   $ git checkout <the revision specified in DEPENDENCIES>


2. Prepare and customize the build
----------------------------------

2.1. Create the build directory. The name is optional and will default
   to 'build', however it helps to choose a name to match the board
   type. For example, we will use axxia.

   $ cd $YOCTO
   $ source meta-intel-axxia/meta-intel-distro/axxia-env
   $ source poky/oe-init-build-env axxia-build

2.2. Check the conf/bblayers.conf file and edit if necessary.

   $ pwd (you should be at $YOCTO/axxia-build)
   $ vi conf/bblayers.conf

BBLAYERS variable should have the following content (references to YOCTO
below shoud be replaced with the actual value you provided prior).

   BBLAYERS ?= " \
            $YOCTO/poky/meta \
            $YOCTO/poky/meta-poky \
            $YOCTO/meta-openembedded/meta-oe \
            $YOCTO/meta-openembedded/meta-networking \
            $YOCTO/meta-openembedded/meta-filesystems \
            $YOCTO/meta-openembedded/meta-python \
            $YOCTO/meta-openembedded/meta-perl \
            $YOCTO/meta-security \
            $YOCTO/meta-security/meta-tpm \
            $YOCTO/meta-ros \
            $YOCTO/meta-virtualization \
            $YOCTO/meta-intel \
            $YOCTO/meta-intel-axxia/meta-intel-bsp \
            $YOCTO/meta-intel-axxia/meta-intel-distro \
            "

2.3. Check the conf/local.conf file and edit if necessary.

   $ vi conf/local.conf

2.3.1. Set distribution configuration to have Axxia specific features.

    DISTRO = "intel-axxia"

2.3.2. Set Release version. Replace it with the tag for meta-intel-axxia
    repository from step 1.1.

    RELEASE_VERSION = "(meta-intel-axxia-tag)"

2.3.3. Select a specific machine to target the build with:

    - for INTEL Axxia x86_64 Gen3 systems (Snow Ridge CPU)
    MACHINE = "intel-axxia-snr"

     - for INTEL Axxia x86_64 Gen4 systems (Grand Ridge CPU)
     MACHINE = "intel-axxia-grr"

     - for INTEL QSP System used in simulation
     MACHINE = "genericx86-64"

2.3.4. Select the main kernel to use (virtual/kernel provider).
     You can build the kernel from 3 sources:

a. Public Intel Github (github.com/intel)

   PREFERRED_PROVIDER_virtual/kernel = "linux-intel"
   
   will build kernel from Intel Github public repos:
   5.10: git://github.com/intel/linux-intel-lts.git
         5.10/yocto or 5.10/preempt-rt
         
   Other versions for the Intel kernel are: linux-intel-rt,
   linux-intel-debug, linux-intel-rt-debug.
   
b. Yocto Git (git.yoctoproject.org)

   PREFERRED_PROVIDER_virtual/kernel = "linux-yocto"
   
   will build kernel from Intel Github public repos:
   5.10: git://git.yoctoproject.org/linux-yocto.git
         v5.10/standard/base or v5.10/standard/preempt-rt/base
         
   Other versions for the Yocto kernel are: linux-yocto-rt,
   linux-yocto-debug, linux-yocto-rt-debug.

   NOTE: For INTEL QSP System used in simulation, linux-yocto is
   the only compatible kernel with genericx86-64 machine.

c. Local Kernel repositories (for Linux development)

   If you want to base a Yocto build on a local Linux kernel, do the
   following. First, make the following changes to local.conf.

   PREFERRED_PROVIDER_virtual/kernel = "linux-local"
   LOCAL_KERNEL_PATH = "<path to local Linux repository>"
   LOCAL_KERNEL_BRANCH = "<branch in the repository>"

   Then, in the meta-intel-distro layer, add a Linux configuration file
   next to linux.loca.bb recipe. Copy a full configuration to
   meta-intel-distro/recipes-kernel/linux/defconfig.

   NOTE: All changes must be committed in the local kernel, as the
         build will clone it using git.

2.3.5. Select the version for the main kernel:

   PREFERRED_VERSION_(preferred-provider)= "5.10%"

NOTE: No preferred version should be set for linux-local provider.

2.3.6. Build multiple versions of the kernel in one build

   By default, with intel-axxia distro, 4 versions of kernel are built
   in one build linux-intel, linux-intel-rt and debug versions for each.
   
   By default, with intel-axxia distro, 4 versions of kernel are built
   in one build depending on the generic provider for virtual/kernel:

   - for any linux-intel* kernel provider, bitbake will build linux-intel,
     linux-intel-rt, linux-intel-debug and linux-intel-rt-debug kernels
   
   - for any linux-yocto* kernel provider, bitbake will build linux-yocto,
     linux-yocto-rt, linux-yocto-debug and linux-yocto-rt-debug kernels
  
   The kernel which is set as virtual/kernel provider will be installed
   as usual in the deploy directory of the build. Alternative kernels
   with their modules will be installed in deploy directory under 
   separate folders having the following template name: kernel-<PN>. 
   
   Alternative kernels will be also installed in rootfs /boot directory
   and the modules for all kernels in /lib/modules/<KENREL_VERSION>.
   Kernels can be swhitch by recreating the bzImage symlink in /boot.
   Default is the kernel set as preferred provider for virtual/kernel.

   If don't want to build alternative kernels, just reset the following
   variable in local.conf:

   ALTERNATIVE_KERNELS = ""

2.3.7. Enable "multilib" support if desired.

    DISTRO_FEATURES_append = " multilib"

2.3.8. Add simics support (for large and sim images). This will install
    simicsfs-client and simics-agent in rootfs. If enabled, it's mandatory
    to set the SIMICS version.

    DISTRO_FEATURES_append = " simics"
    SIMICS_VERSION = "<simics-version>"

    Sources tgz archive should be copied in meta-intel-distro/downloads
    directory: simics-<simics-version>.tgz and should match the version
    set in SIMICS_VERSION.

    To generate simics-<simics-version>.tgz, get "ase_jvl_external*.txz" from
    the SW download page (VIP), and do the following.

    * tar xf ase_jvl_external*.txz
    * 'cd ase/simics'
    * tar czf simics-<simics-version>.tgz simics-<simics-version>

2.3.9. Choose a different GCC version to use: 9.% (gcc9 feature) or 10.%
    (gcc10 feature) via distro features. Default version is 11.%.

    DISTRO_FEATURES_append = " gcc9"
    
2.3.10. Select the version of python3 to use: 3.9 (default), 3.8 or 3.6.

    PYTHON_VERSION = "3.8"
    
2.3.11. Depending on your host processor, set these two options that
      control how much parallelism BitBake should use (optional):

    BB_NUMBER_THREADS = "12"
    PARALLEL_MAKE = "-j 12"

2.3.12. Other optional settings for saving disk space and build time:
   
    DL_DIR = "/(some-shared-location)/downloads"
    SSTATE_DIR = "/(some-shared-location)/sstate-cache

2.4. Examples

    See http://www.yoctoproject.org/docs/2.3/mega-manual/mega-manual.html
    for complete documentation on the Yocto build system.

    Here are the local.conf files used for open builds:

MACHINE = "intel-axxia-snr"
PREFERRED_PROVIDER_virtual/kernel = "linux-intel"
PREFERRED_VERSION_linux-intel = "5.10%"
DISTRO = "intel-axxia"
DISTRO_FEATURES_append = " simics"
PACKAGE_CLASSES ?= "package_rpm"
EXTRA_IMAGE_FEATURES ?= "debug-tweaks"
USER_CLASSES ?= "buildstats image-mklibs image-prelink"
PATCHRESOLVE = "noop"
BB_DISKMON_DIRS = "\
    STOPTASKS,${TMPDIR},1G,100K \
    STOPTASKS,${DL_DIR},1G,100K \
    STOPTASKS,${SSTATE_DIR},1G,100K \
    STOPTASKS,/tmp,100M,100K \
    ABORT,${TMPDIR},100M,1K \
    ABORT,${DL_DIR},100M,1K \
    ABORT,${SSTATE_DIR},100M,1K \
    ABORT,/tmp,10M,1K"
PACKAGECONFIG_append_pn-qemu-native = " sdl"
PACKAGECONFIG_append_pn-nativesdk-qemu = " sdl"
CONF_VERSION = "1"


3. Select the image type and start the build
--------------------------------------------

   $ cd $YOCTO/axxia-build
   $ bitbake (image type)

Available root filesystem types:
   * axxia-image-min
     A small image used in simulation, flash, or as a ram disk. Should
     be sufficient to use the RTE. 

   * axxia-image-run
     An image used in simulation.

   * axxia-image-dev
     A more complete image. Contains the SDK and LTP.

   * axxia-image-qsp (for genericx86-64 machine)
     A small image used for the QSP System used in simulation.

Once complete the images for the target machine will be available in 
the output directory 'tmp/deploy/images/$MACHINE'.

3.1. Images generated:

* (image type)-(machine name).ext2 (rootfs in EXT2 format)
* (image type)-(machine name).ext4 (rootfs in EXT4 format)
* (image type)-(machine name).iso (rootfs in ISO format)
* (image type)-(machine name).tar.gz (rootfs in tar+GZIP format)
* (image type)-(machine name).wic (rootfs in wic format)
* modules-(machine name).tgz (modules in tar+GZIP format)
* bzImage and bzImage-(machine name) (Linux Kernel binary)


4. Build and install the SDK
----------------------------

Run 'populate_sdk' task for axxia-image-dev image to build the SDK.

    $ bitbake axxia-image-dev -c populate_sdk

It will install a self extracting script in tmp/deploy/sdk. Simply run
the poky*.sh script. To set up the environment to use the tools, source
environment-setup* in the install directory.

After the installation completes, do the following:

4.1 Optional Linux Module Tools Update

If external Linux modules need to be buildable after installing the
SDK, the Linux kernel sources need to be updated as follows, using the
external-modules-setup.sh script which is deployed with the SDK.

source (install directory)/environment-setup*
./(install directory)/external-modules-setup.sh

The SDK should contain all you need to run the above. In case the Linux
build fails because of missing packages on the host, be sure you have
the following installed: libncurses5-dev flex bison libssl-dev dkms
libelf-dev (for Ubuntu).


5. Creating Bootable Disks for reference boards
-----------------------------------------------

Here are three ways to boot Victoria Canyon board (VCN) or Travertine
Fall board (TVF) boards.

5.1. SATA or USB based on the .rootfs.wic image.

* Write the .rootfs.wic image to a disk or USB device.
* Clean up the partition table.  Linux utilities such as gdisk and
  parted can do this.
* Attach the device to Victoria Canyon | Travertine Fall board.
* The grub menu should default to "boot", and you should end up at the
  Linux prompt.  By default, the login is root and there is no
  password.

5.2. SATA or USB based on the .iso image.

* Write the .iso image to a disk or USB.
* Attach the device to Victoria Canyon | Travertine Fall board.
* The grub menu should have the following entries.
  * "boot" - This will boot to the Linux prompt with a read-only /.
  * "install" - This will install to another attached device.
  * "Reboot into Firmware interface" - Go back to the EFI menu.

5.3. NETWORK BOOT

Because many servers and services are available, they are not
described here.  The files needed to boot are available.

  * Multiple kernel images are available.  The kernel version selected
    in local.conf will be available as
    <TMPDIR>/deploy/images/<machine>/bzImage with the other versions
    available as <TMPDIR>/deploy/images/<machine>/kernel-*/bzImage.
    Available versions of the kernel are described above.
  * Multiple root file systems are also available.  The contents are
    described above (see axxia-image-min, axxia-image-run, and
    axxia-image-dev).  Various formats are available.


6. Read-only rootfs with writable overlayfs
-------------------------------------------

Having a read-only root file system is useful for many scenarios:
* Separate user specific changes from system configuration
* Allow factory reset, by deleting the user specific changes
* Have a fallback image in case the user specific changes made the root
  file system no longer bootable
* Share root file system to boot multiple boards via NFS

Because some data on the root file system changes on first boot or
while the system is running, just mounting the complete root file
system as read-only breaks many applications.

Instead of having a read-only root files system and symlinking/bind
mounting files and directories that could potentially change while the
system is running to a writable partition, mounting a writable overlay
root file system, that uses a read-only file system as its base and
writes changed data to another writable partition.

Here no investigation of writable files are needed and factory reset
can be done by just deleting all files or formatting the writable
volume. In case when upperdir of the overlay is mounted as tmpfs, all
the changes will be reset on reboot.

In order to boot the rootfs read-only with writable overlayfs, set the
custom init-readonly-overlay script as init script for boot with the
following Kernel command line parameter:

    init=/init-readonly-overlay

This will mount the rootfs read-only in /media/rfs/ro, a tmpfs
mountpoint will be created in /media/rfs/rw and an overlayfs will be
mounted on root / directory using the readonly rootfs as lowerdir and
the tmpfs one as upperdir. All the user specific changes will be stored
in tmpfs, so will be volative.

For persistent user changes, a new read-write device/partition need to
be present and set with the following Kernel command line parameter:

    rootrw=/dev/sda1
    
The lower filesystem can be any filesystem supported by Linux and does
not need to be writable. But, since the upper filesystem will normally
be writable, it must support the creation of trusted.* extended
attributes, and must provide valid d_type in readdir responses, so NFS
is not suitable.

Other Kernel command line parameters:
root= specifies the read-only root file system device. If this is not 
      specified, the current rootfs is used.

rootfstype= if support for the read-only file system is not build into
            the kernel, you can specify the required module name here.
            It will also be used in the mount command.

rootoptions= specifies the mount options of the read-only file system. 
             Defaults to noatime,nodiratime.

rootinit= if the init parameter was set to init-readonly-overlay script,
          rootinit can be used to overwrite the default (/sbin/init).
          E.g. rootinit=/bin/sh

rootrw= specifies the read-write file system device. If this is not
        specified, tmpfs is used.

rootrwfstype= if support for the read-write file system is not build
              into the kernel, you can specify the required module name
              here. It will also be used in the mount command.

rootrwoptions= specifies the mount options of the read-write file
               system. Defaults to rw,noatime.

rootrwreset= set to yes if you want to delete all the files in the
             read-write file system prior to building the overlay root
             files system.

More details on how to setup a writable root file system overlay on
top of a read-only root file system can be found in
meta-readonly-rootfs-overlay layer [1], and details about overlays can
be found in the Kernel official documentation [2].

1. https://github.com/cmhe/meta-readonly-rootfs-overlay
2. https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt
